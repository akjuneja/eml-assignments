{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import datasets",
      "metadata": {
        "trusted": true
      },
      "execution_count": 1,
      "outputs": [],
      "id": "9bfb3fa9-7a5f-4fea-a288-69c365d3ad69"
    },
    {
      "cell_type": "code",
      "source": "class SGDLogisticRegression():\n    def __init__(self,input_dim, output_dim, alpha=0.01,iter=1000):\n        self.weight = np.random.rand(input_dim, output_dim)\n        self.bias = np.random.rand(output_dim)\n        self.alpha = alpha\n        self.iter = iter\n\n    def softmax(self, z):\n        exp_z= np.exp(z-np.max(z))\n        return exp_z/np.sum(exp_z, axis=0)\n    \n    def predict_prob(self,X):\n        z= self.bias+ np.dot(X,self.weight)\n        return self.softmax(z)\n    \n    def classify(self,X):\n        pred_prob= self.predict_prob(X)\n        return np.argmax(pred_prob, axis=1)\n    \n\n    def gradient_update(self, X, Y_o):\n        pred_prob= self.predict_prob(X)\n\n        #gradient with respect to W\n        gradW= np.dot(X.T, (pred_prob-Y_o))/X.shape[0]\n\n        #gradient with respect to b\n        gradB= np.sum(pred_prob-Y_o, axis=0)/X.shape[0]\n\n        #update weight and bias\n        self.weight -= self.alpha*gradW\n        self.bias -= self.alpha*gradB\n\n    \n    def train(self,X,Y):\n        one_hot = OneHotEncoder(sparse_output=False, categories='auto').fit_transform(Y.reshape(-1,1))\n\n        for i in range(self.iter):\n            self.gradient_update(X, one_hot)\n            pass\n        return self",
      "metadata": {
        "trusted": true
      },
      "execution_count": 28,
      "outputs": [],
      "id": "3a9b911c-f884-403a-9e03-2a16658eb3b8"
    },
    {
      "cell_type": "code",
      "source": "dataset = sklearn.datasets.load_digits()\nX_data,y_data = dataset.data, dataset.target\nX = StandardScaler().fit(X_data).transform(X_data) \nfor i in range(10):\n    X_train, X_test, y_train, y_test = train_test_split(X_data,y_data, test_size=0.2,random_state=i)\n\n    sgdlog = SGDLogisticRegression(X_train.shape[1],10,iter=345,alpha=0.009) #TODO Choose appropriate iter and alpha\n    sgdlog = sgdlog.train(X_train,y_train)\n    print(f\"Iteration {i + 1}:\")\n    print(accuracy_score(sgdlog.classify(X_train),y_train ))\n    print(accuracy_score(sgdlog.classify(X_test),y_test))\n    print()",
      "metadata": {
        "trusted": true
      },
      "execution_count": 29,
      "outputs": [
        {
          "name": "stdout",
          "text": "Iteration 1:\n0.732776617954071\n0.7638888888888888\n\nIteration 2:\n0.7258176757132916\n0.7805555555555556\n\nIteration 3:\n0.7627000695894224\n0.7527777777777778\n\nIteration 4:\n0.7035490605427975\n0.7444444444444445\n\nIteration 5:\n0.7425191370911621\n0.7861111111111111\n\nIteration 6:\n0.7418232428670842\n0.8055555555555556\n\nIteration 7:\n0.7515657620041754\n0.7833333333333333\n\nIteration 8:\n0.7814892136395268\n0.7333333333333333\n\nIteration 9:\n0.732776617954071\n0.775\n\nIteration 10:\n0.7710508002783577\n0.6833333333333333\n\n",
          "output_type": "stream"
        }
      ],
      "id": "00b59e83-1101-4cd6-a59f-7849f62422da"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "b5b6e0a6-2803-4c09-ae9f-402f67447944"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "7ab4773d-cd50-4be8-9dd7-93d78b6cda56"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "b35e1c9d-f806-4837-8561-c6c35867f884"
    }
  ]
}